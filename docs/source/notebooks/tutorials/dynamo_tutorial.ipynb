{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Edit module is designed for dynamically editing models during runtime. Under the hood, it uses TorchDynamo to recompile PyTorch code into Torch FX graphs. \n",
    "\n",
    "**TorchDynamo** is a Python-level Just-In-Time (JIT) compiler designed to make unmodified PyTorch programs faster. TorchDynamo hooks into the frame evaluation API in CPython ([PEP 523](https://peps.python.org/pep-0523/)) to dynamically modify Python bytecode right before it is executed. It rewrites Python bytecode to extract sequences of PyTorch operations into an [FX Graph](https://pytorch.org/docs/stable/fx.html) which is then compiled with a customizable backend. It creates this FX Graph through bytecode analysis and is designed to mix Python execution with compiled backends to get the best of both worlds — usability and performance. (*[TorchDynamo Deep Dive](https://pytorch.org/docs/stable/torch.compiler_deepdive.html)*)\n",
    "\n",
    "This notebook will walk through the basic elements of the Edit module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from nnsight.util import WrapperModule\n",
    "from nnsight.edit import print_gm, Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a (relatively) simple torch model to demonstrate how operations are translated into a Torch FX graph. Two things to take note of. \n",
    "\n",
    "First, we declare a simple module `WrappedLayer` to observe what TorchDynamo does when it runs into user defined Torch modules. \n",
    "\n",
    "Second, we declare a couple functions and methods `split` and `x * 100` to illustrate the different operations in a Torch FX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-195.7624]], grad_fn=<SplitBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "class WrappedLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = x * 100\n",
    "        return x\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1, 1)\n",
    "        self.wrapped = WrappedLayer()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.wrapped(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.split(1, dim=-1)\n",
    "        return x\n",
    "\n",
    "mod = M()\n",
    "\n",
    "input_tensor = torch.tensor([[1.0]])\n",
    "output = mod(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Dynamo is the backbone of our Edit module, we'll largely interface with it through the Torch Compile method. It uses Dynamo under the hood to JIT compile arbitrary Python code. Passing a custom backend is an easy interface for editing and viewing FX graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name     target                       args           kwargs\n",
      "-------------  -------  ---------------------------  -------------  -----------\n",
      "placeholder    l_x_     L_x_                         ()             {}\n",
      "call_module    x        L__self___layer1             (l_x_,)        {}\n",
      "call_module    x_1      L__self___wrapped_layer1     (x,)           {}\n",
      "call_function  x_3      <built-in function mul>      (x_1, 100)     {}\n",
      "call_module    x_4      L__self___dropout            (x_3,)         {}\n",
      "call_method    split    split                        (x_4, 1)       {'dim': -1}\n",
      "call_function  getitem  <built-in function getitem>  (split, 0)     {}\n",
      "output         output   output                       ((getitem,),)  {}\n"
     ]
    }
   ],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    \n",
    "    # Example inputs is a list of torch tensors that represent the input to the model\n",
    "    # print(example_inputs)\n",
    "\n",
    "    gm.graph.print_tabular()\n",
    "\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(mod, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the functions and components we declared in the module are translated into nodes and their respective operations in an FX graph. Dynamo will trace through user defined modules such as WrappedLayer, breaking apart the operations on its forward pass into separate nodes on the FX graph.\n",
    "\n",
    "There are several elementary operations. \n",
    "- `placeholder` represents a function input.\n",
    "- `get_attr` retrieves a parameter from the module hierarchy. \n",
    "- `call_function` applies a free function to some values.\n",
    "- `call_module` applies a module in the module hierarchy’s `forward()` method to given arguments. \n",
    "- `call_method` calls a method on a value.\n",
    "- `output` contains the output of the traced function in its `args[0]` attribute.\n",
    "\n",
    "Now, let's see what happens if we load this module into NNsight and trace it. Note how we call `torch._dynamo.reset()` to clear existing backends and optimizations on this module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    x       L_stack0_                ()         {}\n",
      "call_function  x_1     <built-in function mul>  (x, 100)   {}\n",
      "output         output  output                   ((x_1,),)  {}\n",
      "opcode         name     target                       args           kwargs\n",
      "-------------  -------  ---------------------------  -------------  -----------\n",
      "placeholder    x        L_stack0_                    ()             {}\n",
      "call_method    split    split                        (x, 1)         {'dim': -1}\n",
      "call_function  getitem  <built-in function getitem>  (split, 0)     {}\n",
      "output         output   output                       ((getitem,),)  {}\n"
     ]
    }
   ],
   "source": [
    "from nnsight import NNsight\n",
    "\n",
    "nn_model = NNsight(mod)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(nn_model._model, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading our model with NNsight, we find that our Dynamo has produced two separate graphs. When TorchDynamo encounters unsupported Python features, such as data-dependent control flow, it breaks the computation graph, lets the default Python interpreter handle the unsupported code, then resumes capturing the graph. (*From [TorchDynamo Deep Dive](https://pytorch.org/docs/stable/torch.compiler_deepdive.html)*)\n",
    "\n",
    "We can see where TorchDynamo breaks the graph by using `torch._dynamo.explain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Count: 2\n",
      "Graph Break Count: 1\n",
      "Op Count: 2\n",
      "Break Reasons:\n",
      "Ops per Graph:\n",
      "  Ops 1:\n",
      "    <built-in function mul>\n",
      "  Ops 2:\n",
      "    <built-in function getitem>\n",
      "Out Guards:\n",
      "  Guard 1:\n",
      "    Name: \"L['self'].layer1\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: TYPE_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 2:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 3:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 4:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dbf9f77e0; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 5:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 6:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 7:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 8:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 9:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140109419972816)\"]\n",
      "    Object Weakref: <weakref at 0x7f6dad13e7a0; to 'M' at 0x7f6dc43508d0>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dc432de90; to 'type' at 0xa404eb0 (M)>\n",
      "  Guard 10:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 11:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 12:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 13:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 14:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 15:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 16:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 17:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 18:\n",
      "    Name: \"G['detect_fake_mode']\"\n",
      "    Source: global\n",
      "    Create Function: FUNCTION_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 19:\n",
      "    Name: \"L['input']\"\n",
      "    Source: local\n",
      "    Create Function: LIST_LENGTH\n",
      "    Guard Types: ['LIST_LENGTH']\n",
      "    Code List: [\"___check_type_id(L['input'], 8810304)\", \"len(L['input']) == 1\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: <weakref at 0x7f6ead54f6a0; to 'type' at 0x866f40 (tuple)>\n",
      "  Guard 20:\n",
      "    Name: \"L['input'][0]\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['input'][0], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dbf9f77e0; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 21:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 22:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 23:\n",
      "    Name: \"L['self'].wrapped\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 24:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 25:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 26:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 27:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 28:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 29:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 30:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140109419972816)\"]\n",
      "    Object Weakref: <weakref at 0x7f6dad13e7a0; to 'M' at 0x7f6dc43508d0>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dc432de90; to 'type' at 0xa404eb0 (M)>\n",
      "  Guard 31:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 32:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dc3b2dd00; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 33:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 34:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dc3b2dd00; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 35:\n",
      "    Name: \"L['self'].layer1\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: TYPE_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 36:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 37:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 38:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 39:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 40:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 41:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 42:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140109666260560)\"]\n",
      "    Object Weakref: <weakref at 0x7f6dbf96d0d0; to 'WrappedLayer' at 0x7f6dd2e31650>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dab4c97b0; to 'type' at 0xa404ae0 (WrappedLayer)>\n",
      "  Guard 43:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 44:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 45:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 46:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 47:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 48:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 49:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 50:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 51:\n",
      "    Name: \"G['detect_fake_mode']\"\n",
      "    Source: global\n",
      "    Create Function: FUNCTION_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 52:\n",
      "    Name: \"L['input']\"\n",
      "    Source: local\n",
      "    Create Function: LIST_LENGTH\n",
      "    Guard Types: ['LIST_LENGTH']\n",
      "    Code List: [\"___check_type_id(L['input'], 8810304)\", \"len(L['input']) == 1\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: <weakref at 0x7f6ead54f6a0; to 'type' at 0x866f40 (tuple)>\n",
      "  Guard 53:\n",
      "    Name: \"L['input'][0]\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['input'][0], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dc3b2dd00; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 54:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 55:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 56:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 57:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 58:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 59:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 60:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 61:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 62:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dbf9f5f80; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 63:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 64:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 65:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 66:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 67:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 68:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 69:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 70:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140109419972816)\"]\n",
      "    Object Weakref: <weakref at 0x7f6dad13e7a0; to 'M' at 0x7f6dc43508d0>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dc432de90; to 'type' at 0xa404eb0 (M)>\n",
      "  Guard 71:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dc3bc9d00; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 72:\n",
      "    Name: \"L['self'].dropout\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: TYPE_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 73:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 74:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 75:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140109342776896))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 76:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 77:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '40021786b19e902157e2d1176772cf00'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 78:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 79:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 80:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f6dc3ba76f0; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f6dae1b1530; to 'torch._C._TensorMeta' at 0x6d212d0 (Tensor)>\n",
      "  Guard 81:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 82:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "Compile Times: TorchDynamo compilation metrics:\n",
      "Function                         Runtimes (s)\n",
      "-------------------------------  ----------------------------------------------------------------------\n",
      "_compile.<locals>.compile_inner  0.0282, 0.0092, 0.0031, 0.0354, 0.0237, 0.0088, 0.0166, 0.0275, 0.0080\n",
      "OutputGraph.call_user_compiler   0.0078, 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch._dynamo.reset()\n",
    "explain_output = torch._dynamo.explain(nn_model._model)(torch.tensor([[1.0]]))\n",
    "print(explain_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the graph broke once on the multiply `x = x * 100`. We already knew this by looking at the broken graph. \n",
    "\n",
    "We can force TorchDynamo to raise an error upon the first graph break encountered by using `fullgraph=True`. The stack trace will provide more details on exactly what is breaking our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2903989/1892653539.py\", line 5, in <module>\n",
      "    opt_bar(torch.tensor([[1.0]]))\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 655, in catch_errors\n",
      "    return callback(frame, cache_entry, hooks, frame_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 383, in _convert_frame_assert\n",
      "    compiled_product = _compile(\n",
      "                       ^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 646, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 562, in compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 151, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 527, in transform\n",
      "    tracer.run()\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2128, in run\n",
      "    super().run()\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "    and self.step()\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1802, in CALL\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py\", line 702, in call_function\n",
      "    return variables.UserFunctionVariable(fn, source=source).call_function(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "    and self.step()\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1802, in CALL\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/user_defined.py\", line 437, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/base.py\", line 311, in call_function\n",
      "    unimplemented(f\"call_function {self} {args} {kwargs}\")\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/exc.py\", line 193, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(_hook) [UnspecializedNNModuleVariable(Linear), TupleVariable(), ConstDictVariable(), TensorVariable()] {}\n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_2903989/671300478.py\", line 19, in forward\n",
      "    x = self.layer1(x)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1572, in _call_impl\n",
      "    hook_result = hook(self, args, kwargs, result)\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback as tb\n",
    "\n",
    "opt_bar = torch.compile(nn_model._model, fullgraph=True)\n",
    "try:\n",
    "    opt_bar(torch.tensor([[1.0]]))\n",
    "except:\n",
    "    tb.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the error trace reveals this line toward the end. \n",
    "\n",
    "```\n",
    "torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(_hook) [UnspecializedNNModuleVariable(Linear), TupleVariable(), ConstDictVariable(), TensorVariable()] {}\n",
    "```\n",
    "\n",
    "This message indicates Dynamo ran into an unsupported Python feature - some forward_hook - and broke the graph. \n",
    "\n",
    "We can remove NNsight hooks by accessing the underlying `._envoy` and clearing the hooks with `.clear_hooks(propagate=True)`. Propagate tells NNsight to remove the hooks of an envoy's sub_envoys too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                             target                                args                                                                 kwargs\n",
      "-------------  -------------------------------  ------------------------------------  -------------------------------------------------------------------  -----------\n",
      "placeholder    l_x_                             L_x_                                  ()                                                                   {}\n",
      "get_attr       l__self___layer1_weight          L__self___layer1_weight               ()                                                                   {}\n",
      "get_attr       l__self___layer1_bias            L__self___layer1_bias                 ()                                                                   {}\n",
      "call_function  x                                <built-in function linear>            (l_x_, l__self___layer1_weight, l__self___layer1_bias)               {}\n",
      "get_attr       l__self___wrapped_layer1_weight  L__self___wrapped_layer1_weight       ()                                                                   {}\n",
      "get_attr       l__self___wrapped_layer1_bias    L__self___wrapped_layer1_bias         ()                                                                   {}\n",
      "call_function  x_1                              <built-in function linear>            (x, l__self___wrapped_layer1_weight, l__self___wrapped_layer1_bias)  {}\n",
      "call_function  x_3                              <built-in function mul>               (x_1, 100)                                                           {}\n",
      "call_function  x_4                              <function dropout at 0x7f6dadc8bba0>  (x_3, 0.1, True, False)                                              {}\n",
      "call_method    split                            split                                 (x_4, 1)                                                             {'dim': -1}\n",
      "call_function  getitem                          <built-in function getitem>           (split, 0)                                                           {}\n",
      "output         output                           output                                ((getitem,),)                                                        {}\n"
     ]
    }
   ],
   "source": [
    "nn_model._envoy.clear_hooks(propagate=True)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(nn_model._model, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Intervening on the FX Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchDynamo is a powerful tool for compiling torch modules to improve performance and efficiency at scale. \n",
    "\n",
    "https://depyf.readthedocs.io/en/latest/walk_through.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we used TorchCompile to attach arbitrary modules at any point in an existing module's computation? There are a couple obvious benefits: \n",
    "\n",
    "1. Edit models to access arbitrary attributes that aren't normally available.\n",
    "2. Add modules such as dictionaries or lora weights and access the hidden states of those modules - on a forward or backward pass - with hooks. \n",
    "3. We can just host one module on NDIF and use Torch compile to recompile existing modules. Compile simply returns an optimized module wrapper over the existing module, so we don't have to host multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare a simple model to see how we can wrap one of its attributes below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1.8184]], grad_fn=<SplitBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        # We want to wrap `value`\n",
    "        # So we can intervene on it with NNsight hooks\n",
    "        value = x[:,0]\n",
    "        x = x * value\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.split(1, dim=-1)\n",
    "        return x\n",
    "\n",
    "mod = M()\n",
    "\n",
    "input_tensor = torch.tensor([[1.0]])\n",
    "output = mod(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we'd like to access the `value` attribute. We wouldn't normally be able to do this with hooks because its not declared as a module attribute.\n",
    "\n",
    "We create the `WrapperModule` class which just passes an input through itself. By setting it as an attribute of the parent module, we can access the input and output of this wrapper with hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M(\n",
      "  (layer1): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (value_wrapper): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class WrapperModule(torch.nn.Module):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        return args\n",
    "    \n",
    "wrapper_module = WrapperModule()\n",
    "wrapper_name = 'value_wrapper'\n",
    "\n",
    "setattr(mod, wrapper_name, wrapper_module)\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out at what node to insert the model, we can compile and print the graph module. This returns the recompiled bytecode from TorchDynamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_x_ : torch.Tensor):\n",
      "    l_x_ = L_x_\n",
      "    l__self___layer1_weight = self.L__self___layer1_weight\n",
      "    l__self___layer1_bias = self.L__self___layer1_bias\n",
      "    x = torch._C._nn.linear(l_x_, l__self___layer1_weight, l__self___layer1_bias);  l_x_ = l__self___layer1_weight = l__self___layer1_bias = None\n",
      "    value = x[(slice(None, None, None), 0)]\n",
      "    x_1 = x * value;  x = value = None\n",
      "    x_2 = torch.nn.functional.dropout(x_1, 0.1, True, False);  x_1 = None\n",
      "    split = x_2.split(1, dim = -1);  x_2 = None\n",
      "    getitem_1 = split[0];  split = None\n",
      "    return (getitem_1,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, _: List[torch.Tensor]):\n",
    "    print(gm)\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "opt_model = torch.compile(mod, backend=custom_backend, dynamic=True, fullgraph=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "value = x[(slice(None, None, None), 0)]\n",
    "x_1 = x * value;  x = value = None\n",
    "```\n",
    "\n",
    "From these lines, we see that value is:\n",
    "1. A node, with args `x` and `slice(...)` representing some `call_method` operation.\n",
    "2. An argument to the node `x_1`. \n",
    "\n",
    "Let's try wrapping `value` as it's passeed as an arg into `x_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "\n",
    "    if wrapper_name not in gm._modules:\n",
    "        gm.add_submodule(wrapper_name, wrapper_module)\n",
    "\n",
    "    for node in gm.graph.nodes:    \n",
    "        if node.name == \"value\":\n",
    "            with gm.graph.inserting_before(node):\n",
    "                new = gm.graph.create_node(node.op, node.target, args=node.args, kwargs=node.kwargs)\n",
    "                wrapper_node = gm.graph.call_module(wrapper_name, args=(new,))\n",
    "                node.replace_all_uses_with(wrapper_node)\n",
    "                gm.graph.erase_node(node)\n",
    "                  \n",
    "    gm.recompile()\n",
    "    return gm.forward\n",
    "\n",
    "\n",
    "torch._dynamo.reset()\n",
    "opt_model = torch.compile(mod, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot happened above, so let's go through it step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Editing in NNsight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, the Edits passed into an NNsight model are loaded into an NNsight Editor context manager and compiled with TorchDynamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\", dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditModule(torch.nn.Module):\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        value = args * 1000\n",
    "        \n",
    "        return value\n",
    "    \n",
    "edit = Edit(\n",
    "    model._envoy.transformer.h[3].attn._module_path, \n",
    "    \"value\", \n",
    "    \"value_wrapper\",\n",
    "    EditModule()\n",
    ")\n",
    "\n",
    "class WrapperModule(torch.nn.Module):\n",
    "    \"\"\"Simple torch module which passes it's input through. Useful for hooking.\n",
    "    If there is only one argument, returns the first element.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        return args\n",
    "    \n",
    "wrapper_edit = Edit(\n",
    "    model._envoy.transformer.h[3].attn._module_path, \n",
    "    \"query\", \n",
    "    \"query_wrapper\",\n",
    "    WrapperModule()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = [wrapper_edit]\n",
    "\n",
    "model.load_edits(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.9474e-01, -2.9457e-01, -1.7623e-01, -4.0702e-01, -5.0194e-01,\n",
      "           5.3164e-01, -1.0857e+00, -7.0488e-01, -1.0685e+00,  6.2466e-01,\n",
      "          -2.2498e-01,  4.4690e-01, -9.4095e-01,  4.2125e-01, -6.0461e-01,\n",
      "          -6.7794e-01, -1.6197e-01,  1.0716e+00,  1.4526e-01,  4.6085e-01,\n",
      "           6.4771e-02,  5.5589e-01, -1.0751e-01,  1.2204e-02, -5.1981e-01,\n",
      "          -5.8530e-01, -1.0644e-01,  6.0577e-01, -2.9360e-01,  8.3008e-01,\n",
      "           3.0922e-01, -1.0348e-01,  6.6434e-01,  3.5581e-01, -1.0595e+00,\n",
      "           5.0450e-01,  6.8544e-01, -2.0407e-01,  7.2883e-02,  1.1289e+00,\n",
      "          -1.7641e-01,  4.3492e-01, -6.4675e-01,  3.5470e-01, -3.1740e-01,\n",
      "          -8.8479e-01,  7.0754e-01,  1.5963e-01, -2.4453e-02,  1.4798e-01,\n",
      "          -7.1829e-01, -4.3326e-01, -1.2205e-01,  7.7209e-01, -3.1324e-01,\n",
      "          -1.5592e+00, -2.2029e-01, -5.4575e-01, -7.1221e-01, -7.0549e-01,\n",
      "           6.1355e-01,  1.0617e-01,  1.1756e+00, -1.5728e-01, -3.3079e-01,\n",
      "           5.2387e-01, -7.3582e-01,  1.0715e+00, -2.6963e-01, -2.8888e-01,\n",
      "          -1.7328e+00, -1.4382e-01, -1.1301e+00,  1.3869e+00,  5.5098e-01,\n",
      "           7.1600e-02, -2.3860e-02,  3.1172e-01, -9.8747e-01,  8.0747e-01,\n",
      "          -3.9031e-01, -1.2705e+00,  1.5374e+00,  4.8823e-01,  1.9274e-01,\n",
      "           1.4481e+00, -6.4960e-01,  4.9234e-02, -6.7085e-01, -1.0140e+00,\n",
      "          -1.4780e-01, -7.6717e-01, -1.0844e+00, -1.4491e+00, -6.1699e-01,\n",
      "           8.6943e-01,  3.4202e-02, -9.2045e-01, -3.5687e-01,  7.0777e-01,\n",
      "          -1.6182e+00, -7.1687e-01,  1.6620e+00, -9.9156e-01, -1.2504e+00,\n",
      "          -1.0037e-01,  2.8830e-02,  1.6791e-01, -7.9602e-01, -1.8408e-01,\n",
      "           1.3473e+00, -3.5242e-01, -1.3767e+00,  3.0414e-01,  7.9273e-01,\n",
      "          -1.8286e+00, -1.0347e+00, -1.3355e-01, -1.2478e+00,  2.1645e-01,\n",
      "          -5.4996e-01, -4.5057e-01,  1.1305e+00,  2.8972e+00, -6.0638e-01,\n",
      "          -1.0083e+00,  6.6864e-01, -1.6586e-01,  1.5923e-01,  7.0939e-01,\n",
      "           9.8359e-01, -3.6039e-01, -1.6318e-01,  1.8019e+00,  2.3878e-01,\n",
      "          -6.3221e-02,  1.5729e-01,  2.0233e-01, -3.1070e-01,  5.9919e-01,\n",
      "          -2.2085e-01, -2.2290e-01, -4.6596e-01,  2.2888e-01,  1.9197e-01,\n",
      "          -4.5660e-01, -2.1218e-01, -4.7887e-01, -6.6827e-01,  4.9828e-01,\n",
      "           6.2239e-01, -4.1999e-01, -2.8471e-01, -2.4607e-01,  8.2304e-01,\n",
      "           7.3738e-01,  4.0419e-01,  4.4265e-01, -1.9213e-01,  4.0825e-01,\n",
      "          -6.3520e-01, -6.3416e-01, -8.9184e-02, -7.5693e-01,  5.8201e-01,\n",
      "          -6.5146e-02, -5.7322e-01,  2.4218e-01,  9.0657e-01,  3.4621e-02,\n",
      "           3.4320e-01,  3.7221e-01,  7.9972e-02,  2.9484e-01, -3.2754e-01,\n",
      "           2.7377e-01,  6.0902e-01, -5.9578e-01, -1.9344e-01, -1.0039e+00,\n",
      "           4.4425e-01,  5.2727e-01, -3.9243e-01, -2.0764e-01,  4.8079e-01,\n",
      "           8.7299e-02, -4.6368e-01,  2.3198e-02,  1.1609e-01,  1.3284e+00,\n",
      "          -5.9925e-01, -5.8221e-01, -6.1854e-01,  1.3837e-01,  1.7306e+00,\n",
      "          -1.4242e+00,  1.3267e-01,  1.1751e+00, -4.4523e-01,  4.5551e-01,\n",
      "          -1.0568e+00, -1.3601e+00, -6.5884e-01, -6.9426e-01, -7.5811e-01,\n",
      "           3.2509e-02, -6.4557e-01,  6.4819e-02,  6.2471e-02, -9.6227e-01,\n",
      "           3.3293e-01, -1.0108e+00, -4.4604e-01, -1.1058e+00,  1.8781e+00,\n",
      "           5.9266e-01,  3.8256e-01,  1.4062e+00, -9.6375e-01,  1.6654e+00,\n",
      "          -6.7780e-01,  9.8577e-01,  1.6757e+00, -8.8619e-01, -7.3879e-01,\n",
      "          -1.6056e+00,  1.6092e+00, -1.7443e-01, -9.5915e-01,  3.2234e-01,\n",
      "           4.2334e-01,  6.0195e-01, -4.0554e-01,  1.3900e-01, -5.2405e-01,\n",
      "           7.2358e-02, -6.2327e-01, -7.3979e-01,  1.2836e-01, -4.6733e-01,\n",
      "          -3.3200e-01, -1.7563e+00,  1.0898e+00, -1.1559e+00, -7.6349e-01,\n",
      "           6.4930e-01, -2.1544e+00, -7.8754e-01,  3.8999e-01, -8.6487e-01,\n",
      "           8.2115e-01,  4.9506e-01,  7.6535e-02,  9.1825e-01,  1.1032e+00,\n",
      "           3.8673e-01,  6.9107e-01, -6.8724e-01,  2.0210e-01, -1.5255e+00,\n",
      "          -5.9363e-01,  1.0773e-01, -1.0590e-01,  2.9143e-02, -8.5484e-01,\n",
      "          -5.9772e-01, -4.4723e-02, -7.8983e-01, -7.9142e-01,  1.0814e+00,\n",
      "           7.3186e-01,  2.6352e-01, -1.0303e+00,  1.7882e-02, -1.4264e+00,\n",
      "          -9.7437e-01,  3.2323e-01, -1.2284e+00, -6.4266e-01, -1.0655e-01,\n",
      "           3.6068e-01,  4.5823e-01,  9.6884e-01,  5.2773e-01,  1.1885e-01,\n",
      "          -6.9896e-02, -4.1950e-01,  1.3237e+00,  2.9287e-01, -1.9740e-01,\n",
      "           1.4927e-01,  5.8836e-01, -1.3437e-01, -1.0837e-02, -2.3861e-01,\n",
      "          -3.1050e-01,  1.6727e+00, -1.0786e+00,  1.0449e-01, -1.3559e-01,\n",
      "          -5.5758e-01, -6.8208e-01,  1.1587e+00, -1.0299e-01,  2.3368e-01,\n",
      "           8.7766e-01, -4.8100e-01, -2.8500e-01,  3.2010e-01, -1.5270e-01,\n",
      "           3.9002e-01,  1.6694e+00,  8.6323e-01,  1.2456e+00,  4.3895e-01,\n",
      "          -5.9806e-01,  8.3081e-01,  3.7853e-01,  4.4832e-01, -1.0211e+00,\n",
      "          -4.8607e-01, -1.6294e+00, -1.1428e-01, -7.5189e-01,  6.8907e-01,\n",
      "           1.5818e+00, -9.3284e-01,  3.7201e-02,  1.4338e+00, -1.2076e+00,\n",
      "          -1.5475e-01, -1.5111e+00, -1.8836e+00,  1.0714e+00,  2.8463e+00,\n",
      "          -1.5468e+00, -9.2198e-01, -5.5011e-01,  1.4692e+00, -8.9348e-01,\n",
      "          -1.4439e+00,  1.2960e+00, -1.2940e+00, -4.3907e-01,  9.5864e-01,\n",
      "          -1.7204e+00,  1.3846e+00,  3.0120e-01,  1.2770e+00, -2.2739e+00,\n",
      "           2.2165e+00,  4.5347e-01, -1.7059e+00,  7.5314e-03,  6.7872e-01,\n",
      "           1.8995e-01, -1.3612e+00, -2.1565e+00, -1.0932e+00,  1.2161e+00,\n",
      "          -1.5329e+00, -1.6283e+00, -3.1500e-01,  1.6295e+00, -1.1051e+00,\n",
      "           1.5266e+00, -2.9407e+00,  4.3446e-01, -3.9948e-01, -6.8393e-01,\n",
      "          -6.9014e-01,  2.9238e-01,  9.3834e-01, -1.1133e+00,  8.5599e-01,\n",
      "          -5.9576e-01,  1.5811e-02, -3.0499e-01, -3.1022e-02,  1.2244e+00,\n",
      "          -1.1656e+00, -5.4312e-01,  1.4432e+00, -1.6135e+00,  2.2409e-01,\n",
      "          -2.3872e-01, -2.7959e-01, -5.4665e-01, -5.9751e-01,  1.8311e-01,\n",
      "           6.2361e-01, -1.3759e-01,  3.5776e-01,  8.3110e-01,  5.4407e-02,\n",
      "          -8.4606e-01,  4.1522e-01,  2.0260e-01,  5.3130e-01,  4.9783e-01,\n",
      "           1.1248e-01,  5.8841e-02,  2.4458e-01,  6.2830e-01,  2.4390e-01,\n",
      "          -2.9786e-01, -1.2500e+00, -5.6147e-01, -1.1400e+00,  3.1650e-01,\n",
      "           2.1330e-01, -3.5752e-01,  4.2084e-01,  3.6071e-01, -9.8784e-01,\n",
      "           3.5830e-01, -1.3110e+00, -5.6173e-01, -9.1173e-01, -3.0324e-02,\n",
      "          -1.3613e-01, -5.8384e-01, -4.8497e-01, -6.9229e-01,  6.1012e-01,\n",
      "          -8.0459e-01,  7.0785e-01, -5.7463e-01,  3.8691e-01,  2.9756e-01,\n",
      "           4.0862e-01,  1.1058e+00,  6.5534e-01, -2.2055e-01, -7.6719e-01,\n",
      "           1.4205e+00,  3.3717e-01, -2.2465e-01,  3.7888e-01, -6.7485e-02,\n",
      "          -6.3094e-01, -8.2366e-01, -1.0657e+00, -1.3032e-01,  4.0120e-01,\n",
      "          -8.0989e-01,  1.0954e-01,  1.1148e+00,  3.4996e-02, -1.2656e+00,\n",
      "           1.3955e+00, -7.0824e-01, -7.2828e-01,  2.5726e-01, -4.1253e-01,\n",
      "          -2.4562e-01, -4.7646e-01, -2.5133e-01,  1.4330e+00,  8.5679e-01,\n",
      "          -3.0282e-01,  5.7544e-01, -1.2564e-01, -3.8891e-01, -1.2355e+00,\n",
      "          -1.2286e+00,  1.1454e+00,  5.7986e-01, -1.0924e+00, -8.9823e-01,\n",
      "           1.5617e-01, -7.3449e-01, -5.2802e-01, -5.5737e-01, -9.5633e-01,\n",
      "           9.1878e-01, -4.6407e-01, -2.1822e+00, -4.6032e-01, -4.4666e-01,\n",
      "           9.7327e-02,  2.6458e-02, -1.7938e-01,  9.4908e-02,  2.4147e-01,\n",
      "          -1.1003e-02, -2.5433e-01,  5.4939e-01, -5.4397e-01, -3.5924e-01,\n",
      "          -7.7453e-01, -5.6269e-01,  1.6950e-01,  1.1347e+00,  7.9104e-01,\n",
      "          -1.2502e+00, -1.2258e+00, -1.0351e-01,  4.0436e-01, -3.4675e-01,\n",
      "           2.5538e-01, -2.9546e-01,  5.9789e-01,  8.5938e-02, -9.9910e-01,\n",
      "          -2.8022e-01, -1.8508e-01, -4.1328e-01, -4.8151e-01, -5.9791e-01,\n",
      "           5.0983e-01,  1.0190e+00, -2.3177e-01,  6.9086e-01,  1.1832e+00,\n",
      "           3.6865e-01,  3.5626e-01,  1.3002e+00, -3.5176e-01, -1.0646e+00,\n",
      "          -1.6935e-01,  1.8611e+00, -9.3484e-01,  3.0165e-01, -4.6170e-02,\n",
      "           9.3825e-01,  3.8519e-01, -1.8126e-01,  8.3955e-01,  8.4650e-02,\n",
      "          -2.1144e-01,  1.1616e+00,  1.4063e-01,  5.7157e-01,  1.9047e-01,\n",
      "          -2.3185e-01, -3.0874e-01, -3.5098e-01, -2.2016e-01, -4.0129e-01,\n",
      "           5.0228e-01,  7.6053e-02, -4.5585e-01,  1.2900e-01, -5.4872e-01,\n",
      "          -2.8274e-01,  5.7427e-02, -5.8294e-01,  6.7393e-01,  1.5294e-01,\n",
      "           8.0948e-01, -3.9563e-01, -8.2745e-01,  7.4635e-01, -4.0614e-02,\n",
      "          -4.1800e-02,  2.0683e-02, -1.0281e-01, -8.3067e-01, -2.9631e-01,\n",
      "           9.2139e-01, -1.3158e-01,  3.7174e-01, -9.6024e-01,  2.2390e-01,\n",
      "          -2.8238e-01, -1.0406e+00,  3.3558e-01,  9.1403e-02, -2.0357e-02,\n",
      "          -9.7665e-01,  1.5300e-01, -3.4292e-01, -1.6405e-01, -2.3627e-02,\n",
      "           1.4740e-01,  2.7346e-01,  5.2838e-01,  2.7583e-01, -2.6246e-01,\n",
      "           6.3992e-01, -3.6822e-01,  2.4649e-01,  2.0639e-01, -5.0527e-02,\n",
      "          -1.0054e+00, -6.7386e-01,  5.0723e-01, -1.2968e+00,  5.3796e-01,\n",
      "           2.8911e-01, -8.2530e-01,  5.9390e-01, -4.5359e-01, -1.7623e-01,\n",
      "          -4.3988e-01, -1.0501e+00,  2.2759e-01,  3.0390e-03,  7.6577e-01,\n",
      "          -1.0612e+00, -1.1578e-01, -5.0932e-02,  2.8302e-01,  1.1187e-01,\n",
      "          -1.9470e-01, -1.2745e-01,  1.1414e-02, -4.3376e-01, -5.5566e-02,\n",
      "           5.4266e-01,  1.7857e-03,  2.9908e-01,  5.8295e-01,  3.8407e-01,\n",
      "           6.2929e-02,  2.8036e-01,  7.7630e-01,  6.4649e-01, -2.0112e-01,\n",
      "          -7.9518e-01,  3.6789e-01, -7.5658e-01, -4.5441e-01,  3.8835e-02,\n",
      "          -3.5312e-01, -4.2377e-01,  4.6634e-02, -6.4218e-01,  5.0563e-01,\n",
      "          -2.1844e+00,  3.9971e-01, -4.2884e-02,  1.0447e-01,  9.9986e-01,\n",
      "          -5.0893e-01,  3.2538e-01, -7.5686e-01,  4.4693e-01,  2.3096e-01,\n",
      "          -3.3906e-01,  8.2843e-01,  1.4376e-01,  4.6295e-01,  2.3694e-01,\n",
      "           6.6796e-01, -6.3151e-01,  4.0659e-01,  4.1135e-01,  7.3255e-02,\n",
      "          -3.8528e-01, -3.8111e-01,  3.0801e-01, -3.4698e-01,  7.4621e-01,\n",
      "          -7.3729e-01, -3.4893e-01, -9.7589e-01,  5.9701e-01,  2.6277e-01,\n",
      "          -9.1164e-01,  2.8440e-01,  8.2000e-01,  5.7775e-01,  2.3409e-01,\n",
      "          -2.4505e-01,  3.6177e-01,  7.8367e-02,  1.0854e-01, -1.2063e-01,\n",
      "          -6.5858e-01,  7.4630e-01, -1.3371e-01,  9.3207e-03,  3.0860e-01,\n",
      "          -1.5669e-01,  8.8602e-02, -4.9325e-01,  3.3118e-01, -3.4853e-01,\n",
      "           1.2283e-01, -1.0392e+00, -1.2344e-01, -2.0905e-01, -8.1793e-01,\n",
      "          -1.6988e-01, -2.6698e-01,  9.0156e-01, -5.4444e-01,  7.3923e-03,\n",
      "          -7.5379e-01, -4.2945e-03,  3.9944e-01,  7.5645e-02, -7.3624e-01,\n",
      "          -1.2677e+00, -9.2028e-02,  6.8868e-01, -3.3162e-01,  2.6183e-01,\n",
      "           2.8232e-01, -2.9840e-03,  8.5342e-02, -5.9206e-01, -3.6830e-01,\n",
      "          -4.1260e-01,  6.3020e-01,  8.5795e-01, -7.2541e-01,  2.3046e-01,\n",
      "           3.9010e-01, -5.7812e-01,  5.1842e-01,  5.3075e-01, -8.0336e-01,\n",
      "          -2.1635e-01, -2.1163e-01, -5.6073e-02,  6.4656e-01, -4.0760e-01,\n",
      "          -7.4416e-01,  5.2597e-01,  1.7210e-01, -1.6159e-01,  3.2315e-02,\n",
      "           1.1118e+00, -8.5661e-01,  7.9577e-01,  2.0606e-01, -6.7927e-01,\n",
      "           3.2077e-02,  2.3751e-01, -4.7496e-01, -8.0359e-01,  4.3499e-02,\n",
      "          -6.4600e-01,  4.3904e-01,  6.3420e-01,  4.3799e-01, -7.7768e-01,\n",
      "          -3.1361e-01, -1.3178e-01,  1.6372e-01, -7.8290e-01,  4.6827e-01,\n",
      "           8.5454e-01, -8.7752e-01,  1.0266e+00,  1.2472e+00, -1.9292e-01,\n",
      "           4.0510e-01,  8.2641e-02, -1.1040e-01, -5.3817e-01,  5.6838e-02,\n",
      "           8.3988e-01,  1.8735e-01,  6.1837e-01,  5.9772e-01, -5.6735e-01,\n",
      "          -1.8478e-01, -2.3961e-01, -1.5530e-02,  8.0570e-01, -1.0918e+00,\n",
      "          -7.8785e-01, -6.0619e-02, -1.0008e+00]]], device='cuda:0',\n",
      "       grad_fn=<SplitBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"empty\", scan=False, validate=False):\n",
    "    query = model.transformer._orig_mod.h[3].attn.query_wrapper.output.save()\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\", dispatch=True)\n",
    "\n",
    "with model.trace(\"empty\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GraphModule()\n",
       "def forward(self, L_x_ : torch._subclasses.fake_tensor.FakeTensor):\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">l_x_</span> = L_x_\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">size</span> = l_x_.<span style=\"color: #008000; text-decoration-color: #008000\">size</span>()\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___bias</span> = self.L__self___bias\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">size_1</span> = l_x_.size(-1)\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">view</span> = l_x_.<span style=\"color: #008000; text-decoration-color: #008000\">view</span>(-1, size_1);  l_x_ = size_1 = None\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___weight</span> = self.L__self___weight\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">x</span> = torch.addmm(l__self___bias, view, l__self___weight);  l__self___bias = view = l__self___weight = None\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">x_1</span> = x.view((1, 1, 768));  x = None\n",
       "    <span style=\"color: #af00ff; text-decoration-color: #af00ff\">return</span> (x_1,)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GraphModule()\n",
       "def forward(self, L_x_ : torch._subclasses.fake_tensor.FakeTensor):\n",
       "    \u001b[33ml_x_\u001b[0m = L_x_\n",
       "    \u001b[32msize\u001b[0m = l_x_.\u001b[32msize\u001b[0m()\n",
       "    \u001b[38;5;208ml__self___bias\u001b[0m = self.L__self___bias\n",
       "    \u001b[32msize_1\u001b[0m = l_x_.size(-1)\n",
       "    \u001b[32mview\u001b[0m = l_x_.\u001b[32mview\u001b[0m(-1, size_1);  l_x_ = size_1 = None\n",
       "    \u001b[38;5;208ml__self___weight\u001b[0m = self.L__self___weight\n",
       "    \u001b[38;5;27mx\u001b[0m = torch.addmm(l__self___bias, view, l__self___weight);  l__self___bias = view = l__self___weight = None\n",
       "    \u001b[32mx_1\u001b[0m = x.view((1, 1, 768));  x = None\n",
       "    \u001b[38;5;129mreturn\u001b[0m (x_1,)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_envoy = model._envoy.transformer.h[3].mlp.c_proj\n",
    "\n",
    "print_gm(attn_envoy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
