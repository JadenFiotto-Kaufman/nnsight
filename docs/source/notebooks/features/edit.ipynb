{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from nnsight.edit import Edit\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some dispatched model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\", dispatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple wrapper module that passes its args through. Useful for hooking the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModule(torch.nn.Module):\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "        \n",
    "        return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare an edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_one = Edit(\n",
    "    \"transformer.h.0.attn\", # Path to module that we want to edit\n",
    "    \"query\", # Name of parameter we want to wrap\n",
    "    \"query_wrapper\", # Name of wrapper in our module\n",
    "    WrapperModule() # Module we'd like to wrap with\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also modify the args as they are passed through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditModule(torch.nn.Module):\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "        \n",
    "        value = args * 100\n",
    "\n",
    "        return value\n",
    "\n",
    "edit_two = Edit(\n",
    "    \"transformer.h.0.attn\", # Path to module that we want to edit\n",
    "    \"attn_weights\", # Name of parameter we want to wrap\n",
    "    \"weights_wrapper\", # Name of wrapper in our module\n",
    "    WrapperModule() # Module we'd like to wrap with\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use various utils from edit module to see how Dynamo will transform bytecode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GraphModule(\n",
       "  (L__self___dropout): Dropout(p=0.1, inplace=False)\n",
       ")\n",
       "def forward(self, L_hidden_states_ : torch.Tensor):\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">l_hidden_states_</span> = L_hidden_states_\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">size</span> = l_hidden_states_.<span style=\"color: #008000; text-decoration-color: #008000\">size</span>()\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___c_fc_bias</span> = self.L__self___c_fc_bias\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">size_1</span> = l_hidden_states_.size(-1)\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">view</span> = l_hidden_states_.<span style=\"color: #008000; text-decoration-color: #008000\">view</span>(-1, size_1);  l_hidden_states_ = size_1 = None\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___c_fc_weight</span> = self.L__self___c_fc_weight\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">x</span> = torch.addmm(l__self___c_fc_bias, view, l__self___c_fc_weight);  l__self___c_fc_bias = view = \n",
       "l__self___c_fc_weight = None\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">hidden_states</span> = x.view((1, 1, 3072));  x = None\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">mul</span> = 0.5 * hidden_states\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">pow_1</span> = torch.pow(hidden_states, 3.0)\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">mul_1</span> = 0.044715 * pow_1;  pow_1 = None\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">add</span> = hidden_states + mul_1;  hidden_states = mul_1 = None\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">mul_2</span> = 0.7978845608028654 * add;  add = None\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">tanh</span> = torch.<span style=\"color: #005fff; text-decoration-color: #005fff\">tanh</span>(mul_2);  mul_2 = None\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">add_1</span> = 1.0 + tanh;  tanh = None\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">hidden_states_1</span> = mul * add_1;  mul = add_1 = None\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___c_proj_bias</span> = self.L__self___c_proj_bias\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">view_2</span> = hidden_states_1.view(-1, 3072);  hidden_states_1 = None\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___c_proj_weight</span> = self.L__self___c_proj_weight\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">x_2</span> = torch.addmm(l__self___c_proj_bias, view_2, l__self___c_proj_weight);  l__self___c_proj_bias = view_2 = \n",
       "l__self___c_proj_weight = None\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">hidden_states_2</span> = x_2.view((1, 1, 768));  x_2 = None\n",
       "    <span style=\"color: #800000; text-decoration-color: #800000\">hidden_states_3</span> = self.L__self___dropout(hidden_states_2);  hidden_states_2 = None\n",
       "    <span style=\"color: #af00ff; text-decoration-color: #af00ff\">return</span> (hidden_states_3,)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GraphModule(\n",
       "  (L__self___dropout): Dropout(p=0.1, inplace=False)\n",
       ")\n",
       "def forward(self, L_hidden_states_ : torch.Tensor):\n",
       "    \u001b[33ml_hidden_states_\u001b[0m = L_hidden_states_\n",
       "    \u001b[32msize\u001b[0m = l_hidden_states_.\u001b[32msize\u001b[0m()\n",
       "    \u001b[38;5;208ml__self___c_fc_bias\u001b[0m = self.L__self___c_fc_bias\n",
       "    \u001b[32msize_1\u001b[0m = l_hidden_states_.size(-1)\n",
       "    \u001b[32mview\u001b[0m = l_hidden_states_.\u001b[32mview\u001b[0m(-1, size_1);  l_hidden_states_ = size_1 = None\n",
       "    \u001b[38;5;208ml__self___c_fc_weight\u001b[0m = self.L__self___c_fc_weight\n",
       "    \u001b[38;5;27mx\u001b[0m = torch.addmm(l__self___c_fc_bias, view, l__self___c_fc_weight);  l__self___c_fc_bias = view = \n",
       "l__self___c_fc_weight = None\n",
       "    \u001b[32mhidden_states\u001b[0m = x.view((1, 1, 3072));  x = None\n",
       "    \u001b[38;5;27mmul\u001b[0m = 0.5 * hidden_states\n",
       "    \u001b[38;5;27mpow_1\u001b[0m = torch.pow(hidden_states, 3.0)\n",
       "    \u001b[38;5;27mmul_1\u001b[0m = 0.044715 * pow_1;  pow_1 = None\n",
       "    \u001b[38;5;27madd\u001b[0m = hidden_states + mul_1;  hidden_states = mul_1 = None\n",
       "    \u001b[38;5;27mmul_2\u001b[0m = 0.7978845608028654 * add;  add = None\n",
       "    \u001b[38;5;27mtanh\u001b[0m = torch.\u001b[38;5;27mtanh\u001b[0m(mul_2);  mul_2 = None\n",
       "    \u001b[38;5;27madd_1\u001b[0m = 1.0 + tanh;  tanh = None\n",
       "    \u001b[38;5;27mhidden_states_1\u001b[0m = mul * add_1;  mul = add_1 = None\n",
       "    \u001b[38;5;208ml__self___c_proj_bias\u001b[0m = self.L__self___c_proj_bias\n",
       "    \u001b[32mview_2\u001b[0m = hidden_states_1.view(-1, 3072);  hidden_states_1 = None\n",
       "    \u001b[38;5;208ml__self___c_proj_weight\u001b[0m = self.L__self___c_proj_weight\n",
       "    \u001b[38;5;27mx_2\u001b[0m = torch.addmm(l__self___c_proj_bias, view_2, l__self___c_proj_weight);  l__self___c_proj_bias = view_2 = \n",
       "l__self___c_proj_weight = None\n",
       "    \u001b[32mhidden_states_2\u001b[0m = x_2.view((1, 1, 768));  x_2 = None\n",
       "    \u001b[31mhidden_states_3\u001b[0m = self.L__self___dropout(hidden_states_2);  hidden_states_2 = None\n",
       "    \u001b[38;5;129mreturn\u001b[0m (hidden_states_3,)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nnsight.edit import print_gm\n",
    "\n",
    "with model.trace(\" \"):\n",
    "    mlp_input = model.transformer.h[0].mlp.input.save()\n",
    "\n",
    "print_gm(model.transformer.h[0].mlp, mlp_input[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the edits into the module, and access at the points we declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_edits([edit_one, edit_two])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first time running a model after loading edits will take a little longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n",
      "((torch.Size([1, 12, 6, 6]),), {})\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"National Deep Inferencing Fabric\", scan=False, validate=False):\n",
    "    value_one = model.transformer.h[0].attn.query_wrapper.output.save()\n",
    "    value_two = model.transformer.h[0].attn.weights_wrapper.input.save()\n",
    "\n",
    "print(value_one.shape)\n",
    "print(value_two.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit and intervene as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(\"National Deep Inferencing Fabric\", scan=False, validate=False):\n",
    "    value_one = model.transformer.h[0].attn.query_wrapper.output.grad.save()\n",
    "    value_two = model.transformer.h[0].attn.weights_wrapper.output * 100\n",
    "\n",
    "    value = model.output.logits.sum()\n",
    "    value.backward()\n",
    "\n",
    "print(value_one.shape)\n",
    "print(value_two.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit has an option kwarg, \"instance\", which takes the instance at which you want to intervene. \n",
    "\n",
    "When Dynamo has a graph break, this makes it so that variable names are reset. By default, the edit module grabs the value from the first graph it is found in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_one = Edit(\n",
    "    \"transformer.h.0.attn\", # Path to module that we want to edit\n",
    "    \"query\", # Name of parameter we want to wrap\n",
    "    \"query_wrapper\", # Name of wrapper in our module\n",
    "    WrapperModule() # Module we'd like to wrap with\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
